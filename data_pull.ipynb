{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling list of applicable races, then using the PCS python API (unofficial) to pull results for those races for the last X years. Applicable races will be [UCI Pro and above](https://www.procyclingstats.com/races.php?season=2023&category=1&racelevel=2&pracelevel=smallerorequal&racenation=&class=&filter=Filter&p=uci&s=calendar-plus-filters), plus [national champs](https://www.procyclingstats.com/races.php?season=2023&category=1&racelevel=3&pracelevel=smallerorequal&racenation=&class=NC&filter=Filter&p=uci&s=calendar-plus-filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from procyclingstats import Race, Stage\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_races(year):\n",
    "    '''\n",
    "    Function to get a list of race URLs for a given year\n",
    "    '''\n",
    "    # URLs of the page to scrape - Pro Tour up and national championships\n",
    "    prefix = 'https://www.procyclingstats.com/races.php?season='\n",
    "    suffixes = [\n",
    "        '&category=1&racelevel=3&pracelevel=smallerorequal&racenation=&class=NC&filter=Filter&p=uci&s=calendar-plus-filters',\n",
    "        '&category=1&racelevel=2&pracelevel=smallerorequal&racenation=&class=&filter=Filter&p=uci&s=calendar-plus-filters'\n",
    "    ]\n",
    "    urls = [prefix + str(year) + suffix for suffix in suffixes]\n",
    "\n",
    "    # Iterate over the URLs\n",
    "    race_urls = []\n",
    "    for url in urls:\n",
    "        # Send a GET request to the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all race entries\n",
    "        race_entries = soup.select('table tr a[href]')\n",
    "\n",
    "        # Extract race names and URLs\n",
    "        for entry in race_entries:\n",
    "            race_url = 'https://www.procyclingstats.com' + entry['href']\n",
    "            race_urls.append(race_url)\n",
    "\n",
    "    race_urls = list(set(race_urls)) # Drop duplicates, in case\n",
    "    race_urls = [race_url[31:] for race_url in race_urls] # Drop the beginning boilerplate from the URL - API doesn't need it\n",
    "\n",
    "    return race_urls\n",
    "\n",
    "def get_race_info(race_url, verbose=True):\n",
    "    '''\n",
    "    Function to get a list of stage URLs plus extra variables from the race page\n",
    "    https://procyclingstats.readthedocs.io/en/latest/api.html#procyclingstats.race_scraper.Race\n",
    "    Returns a dataframe with a row for each stage\n",
    "    '''\n",
    "    # TODO this should use a race_variables var to set variables, as in the below functions\n",
    "    race = Race(race_url)\n",
    "    stage_urls = {\n",
    "        'stage_url': [d['stage_url'] for d in race.stages()] or [f\"{race_url}/result\"] # elegance\n",
    "    }\n",
    "    extra_vars = {\n",
    "        'race_name': [race.name()],\n",
    "        'race_category': [race.category()],  \n",
    "        'uci_tour': [race.uci_tour()]\n",
    "    }\n",
    "    if(verbose):\n",
    "        print(f\"{extra_vars['race_name'][0]}\")\n",
    "    output = pd.merge(pd.DataFrame(extra_vars), pd.DataFrame(stage_urls), how='cross')\n",
    "    return output\n",
    "\n",
    "def fill_stage_info(race_df):\n",
    "    '''\n",
    "    Function to fill in the extra variables for each stage.\n",
    "    Iterates through URLs. Unpacking results happens later.\n",
    "    '''\n",
    "    # Drop troubling races\n",
    "    race_df = race_df[~race_df['stage_url'].str.contains('qinghai')]\n",
    "\n",
    "    # List of variables we want from each stage\n",
    "    stage_variables = ['is_one_day_race', 'distance', 'stage_type', 'winning_attack_length', \n",
    "                       'date', 'won_how', 'avg_speed_winner', 'avg_temperature', \n",
    "                       'vertical_meters', 'profile_icon', 'profile_score', \n",
    "                       'race_startlist_quality_score', 'results','gc','points','kom','youth','teams']\n",
    "    \n",
    "    # Preallocate an empty DataFrame with the same number of rows as race_df\n",
    "    stage_info_df = pd.DataFrame(index=race_df.index, columns=stage_variables)\n",
    "    \n",
    "    for i, stage_url in tqdm(enumerate(race_df['stage_url'])):\n",
    "        # print(f\"Filling stage info for {stage_url}\")\n",
    "\n",
    "        # Initialize selected_stage as empty\n",
    "        selected_stage = pd.DataFrame({key: [None] for key in stage_variables})\n",
    "\n",
    "        try:\n",
    "            stage = Stage(stage_url)\n",
    "        except:\n",
    "            print(f\"Error retrieving stage {i} at {stage_url}\")\n",
    "            # Assign the collected information to the preallocated DataFrame\n",
    "            stage_info_df.iloc[i] = selected_stage.iloc[0]\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Parse the stage information\n",
    "            parsed_stage = stage.parse()\n",
    "            selected_stage.update({key: [parsed_stage[key]] for key in parsed_stage if key in stage_variables})\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing stage {i} at {stage_url}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Get results\n",
    "            selected_stage['results'] = [stage.results()]\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving results for stage {i} at {stage_url}: {e}\")\n",
    "        \n",
    "        # Assign the collected information to the preallocated DataFrame\n",
    "        stage_info_df.iloc[i] = selected_stage.iloc[0]\n",
    "    \n",
    "    # Concatenate the original race_df with the stage_info_df\n",
    "    combined_df = pd.concat([race_df, stage_info_df], axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def expand_results(filled_stage_df):\n",
    "    '''\n",
    "    Function to expand the main results column + secondary results columns\n",
    "    '''\n",
    "    # list of variables we want from main results\n",
    "    result_variables = ['rider_name', 'rider_url', 'team_name', 'team_url', 'rank','status','time','uci_points','pcs_points', 'age']\n",
    "    secondary_result_types = ['gc', 'points', 'kom', 'youth']  # 'teams' is not included here - need to add later as can't join onto rider_url\n",
    "    secondary_result_variables = ['rank','uci_points']\n",
    "    \n",
    "    df = pd.DataFrame(columns= list(filled_stage_df).extend(result_variables))\n",
    "\n",
    "    # Expand main results (stage-rider level)\n",
    "    print(\"Expanding main results\")\n",
    "    for i, row in tqdm(filled_stage_df.iterrows(),total=filled_stage_df.shape[0]):\n",
    "        results = row['results']\n",
    "        stage_info = pd.DataFrame(row).T.reset_index(drop=True)\n",
    "\n",
    "        if isinstance(results, str):\n",
    "            results = ast.literal_eval(results)\n",
    "        if not isinstance(results, list):\n",
    "            continue\n",
    "\n",
    "        # Expand main results\n",
    "        for result in results:\n",
    "            selected_result = pd.DataFrame({key: [value] for key, value in result.items() if key in result_variables})\n",
    "            full_row = pd.concat([stage_info, selected_result], axis=1).drop(columns=['results'])\n",
    "            df = pd.concat([df, full_row], ignore_index=True)\n",
    "\n",
    "    # Expand secondary results (gc, points, kom, youth, teams)\n",
    "    print(\"Expanding secondary results\")\n",
    "    for result_type in secondary_result_types:\n",
    "        expanded_secondary_results = pd.DataFrame()  # Temp dataframe to store expanded rows for each type\n",
    "\n",
    "        for i, row in tqdm(filled_stage_df.iterrows()):\n",
    "            results = row[result_type]\n",
    "            if isinstance(results, str):\n",
    "                results = ast.literal_eval(results)\n",
    "            if not isinstance(results, list):\n",
    "                continue\n",
    "\n",
    "            for result in results:\n",
    "                selected_result = pd.DataFrame(\n",
    "                    {f\"{result_type}_{key}\": [value] for key, value in result.items() if key in secondary_result_variables}\n",
    "                )\n",
    "                selected_result['stage_url'] = row['stage_url']\n",
    "                selected_result['rider_url'] = result.get('rider_url')\n",
    "                expanded_secondary_results = pd.concat([expanded_secondary_results, selected_result], ignore_index=True)\n",
    "\n",
    "        # Merge the accumulated secondary results for the current type into the main dataframe\n",
    "        df = pd.merge(df, expanded_secondary_results, on=['stage_url', 'rider_url'], how='left').drop(result_type, axis=1)\n",
    "\n",
    "    df = df.drop('teams', axis=1)  # Drop teams for now\n",
    "\n",
    "    return df\n",
    "\n",
    "def pull_race_data(years):\n",
    "    '''\n",
    "    Function to pull all data for a given year or years\n",
    "    '''\n",
    "    combined_df = pd.DataFrame()\n",
    "    for year in years:\n",
    "        df = pd.DataFrame()\n",
    "        print(f\"Pulling races for {year}\")\n",
    "        race_urls = get_races(year)\n",
    "        for race_url in tqdm(race_urls):\n",
    "            df = pd.concat([df, get_race_info(race_url,verbose=False)], ignore_index=True)\n",
    "        df.to_csv(f\"race_df_{year}.csv\", index=False)\n",
    "        print(f\"Filling stages for {year}\")\n",
    "        df = fill_stage_info(df)\n",
    "        df.to_csv(f\"stage_df_{year}.csv\", index=False)\n",
    "        print(f\"Expanding results for {year}\")\n",
    "        df = expand_results(df)\n",
    "        df.to_csv(f\"results_df_{year}.csv\", index=False)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    combined_df.to_csv(f\"results_df_{years[0]}_{years[-1]}.csv\", index=False)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run whole setup for 2021-2023 with saves\n",
    "results_df_2021_2023 = pull_race_data([2021,2022,2023])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
